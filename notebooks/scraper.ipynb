{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, num_jobs, verbose):\n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "\n",
    "    # Initializing the webdriver\n",
    "    service = Service(executable_path='./chromedriver.exe')\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "    \n",
    "    # Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(options=options, service=service)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = f'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"{keyword}\"&sc.locationSeoString=Riyadh+%28Saudi+Arabia%29&locId=3110290&locT=C'\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:  # If true, should be still looking for new jobs.\n",
    "\n",
    "        time.sleep(4)  # Let the page load\n",
    "\n",
    "        # Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"selected\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"ModalStyle__xBtn___29PT9\").click()  # Close the sign-up modal\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Going through each job in this page\n",
    "        job_cards = driver.find_elements(By.CLASS_NAME, \"jobCard\")  # Updated to match the current HTML structure\n",
    "        \n",
    "        for job_card in job_cards:  \n",
    "            print(f\"Progress: {len(jobs)}/{num_jobs}\")\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_title = job_card.find_element(By.CLASS_NAME, \"JobCard_jobTitle___7I6y\").text\n",
    "                company_name = job_card.find_element(By.CLASS_NAME, \"EmployerProfile_compactEmployerName__LE242\").text\n",
    "                location = job_card.find_element(By.CLASS_NAME, \"JobCard_location__rCz3x\").text\n",
    "                job_description = job_card.find_element(By.CLASS_NAME, \"JobCard_jobDescriptionSnippet__yWW8q\").text\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to collect job data: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                salary_estimate = job_card.find_element(By.CLASS_NAME, \"JobCard_salaryEstimate__arV5J\").text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1\n",
    "\n",
    "            try:\n",
    "                rating = job_card.find_element(By.CLASS_NAME, \"EmployerProfile_ratingContainer__ul0Ef\").text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1\n",
    "\n",
    "            # Printing for debugging\n",
    "            if verbose:\n",
    "                print(f\"Job Title: {job_title}\")\n",
    "                print(f\"Salary Estimate: {salary_estimate}\")\n",
    "                print(f\"Job Description: {job_description[:500]}\")\n",
    "                print(f\"Rating: {rating}\")\n",
    "                print(f\"Company Name: {company_name}\")\n",
    "                print(f\"Location: {location}\")\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Rating\": rating,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location\n",
    "            })\n",
    "\n",
    "        # Clicking on the \"next page\" button\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, './/li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Scraping terminated before reaching target number of jobs. Needed {num_jobs}, got {len(jobs)}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_db():\n",
    "    print(os.getenv('DATABASE_URL'))\n",
    "    return psycopg2.connect(os.getenv('DATABASE_URL'))\n",
    "\n",
    "\n",
    "def convert_numpy_types(series):\n",
    "    return series.map(lambda x: x.item() if isinstance(x, np.generic) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to insert job data into PostgreSQL\n",
    "def insert_jobs_to_db(jobs_df):\n",
    "    # Convert numpy types to native Python types for each column\n",
    "    for col in jobs_df.columns:\n",
    "        jobs_df[col] = convert_numpy_types(jobs_df[col])\n",
    "    \n",
    "    # Establish the connection\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Define the insert query\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO job_listings (job_title, salary_estimate, job_description, rating, company_name, location)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (job_title, company_name, location) DO NOTHING;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Insert each row into the database\n",
    "    for row in jobs_df.itertuples(index=False):\n",
    "        print(f\"Inserting row: {row[0]}\")\n",
    "        cursor.execute(insert_query, row)\n",
    "    \n",
    "    # Commit and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/20\n",
      "Job Title: Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: 3-5 years of experience as a data engineer. Ensure data integrity and quality throughout the data lifecycle. Optimize data workflows and storage solutions.…\n",
      "Rating: 3.0\n",
      "Company Name: Jadeer\n",
      "Location: Riyadh\n",
      "Progress: 1/20\n",
      "Job Title: Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Implement data quality control measures and data validation processes to ensure the integrity and reliability of the data. Data Integration and ETL Development.…\n",
      "Rating: -1\n",
      "Company Name: webook.com\n",
      "Location: Riyadh\n",
      "Progress: 2/20\n",
      "Job Title: AI Data Engineer - Arabic Speaker\n",
      "Salary Estimate: -1\n",
      "Job Description: Experience with data cleaning, data wrangling, and building data pipelines. Implement data cleaning and data wrangling processes to ensure high-quality data for……\n",
      "Rating: -1\n",
      "Company Name: TechTalent\n",
      "Location: Riyadh\n",
      "Progress: 3/20\n",
      "Job Title: Data Engineer (KSA) - WSP\n",
      "Salary Estimate: -1\n",
      "Job Description: Integrate data from multiple data sources in one dashboard. 5 to 8 years of experience in data visualization and data engineering role.…\n",
      "Rating: -1\n",
      "Company Name: Talent Pal\n",
      "Location: Riyadh\n",
      "Progress: 4/20\n",
      "Job Title: Data Engineer (KSA)\n",
      "Salary Estimate: -1\n",
      "Job Description: Integrate data from multiple data sources in one dashboard. 5 to 8 years of experience in data visualization and data engineering role.…\n",
      "Rating: 3.7\n",
      "Company Name: WSP\n",
      "Location: Riyadh\n",
      "Progress: 5/20\n",
      "Job Title: Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Collaborating with the data scientists to provide data insights for ML model development. Collaborating with other teams, including product managers, data……\n",
      "Rating: 3.9\n",
      "Company Name: Mozn\n",
      "Location: Riyadh\n",
      "Progress: 6/20\n",
      "Job Title: Senior Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Data Integration: Design and implement data ingestion processes from various data sources into our data warehouse.…\n",
      "Rating: 4.7\n",
      "Company Name: Nice One\n",
      "Location: Riyadh\n",
      "Progress: 7/20\n",
      "Job Title: Geospatial data engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Analyze and interpret complex geospatial data sets, and provide recommendations for data collection and analysis.…\n",
      "Rating: 2.9\n",
      "Company Name: Arabic Computer Systems\n",
      "Location: Riyadh\n",
      "Progress: 8/20\n",
      "Job Title: Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: You will use various methods to transform raw data into useful data systems. Technical expertise with data models, data mining, and segmentation techniques.…\n",
      "Rating: 2.9\n",
      "Company Name: Arabic Computer Systems\n",
      "Location: Riyadh\n",
      "Progress: 9/20\n",
      "Job Title: Senior Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Troubleshoot complex data issues and provide effective solutions. Implement data integration solutions using ETL processes and tools.…\n",
      "Rating: 4.1\n",
      "Company Name: Giza Systems EG\n",
      "Location: Riyadh\n",
      "Progress: 10/20\n",
      "Job Title: Senior Big Data Engineer\n",
      "Salary Estimate: -1\n",
      "Job Description: Ensure data quality, integrity, and security across big data platforms. Lead efforts in data modeling, schema design, and data warehousing.…\n",
      "Rating: 4.1\n",
      "Company Name: Giza Systems EG\n",
      "Location: Riyadh\n",
      "Progress: 11/20\n",
      "Job Title: Senior Data Consultant\n",
      "Salary Estimate: -1\n",
      "Job Description: Expertise in designing and implementing data platforms and architectures, including data lakes, data warehouses.…\n",
      "Rating: 4.0\n",
      "Company Name: Devoteam\n",
      "Location: Riyadh\n",
      "Progress: 12/20\n",
      "Job Title: Senior Data Consultant\n",
      "Salary Estimate: -1\n",
      "Job Description: Expertise in designing and implementing data platforms and architectures, including data lakes, data warehouses.…\n",
      "Rating: 4.0\n",
      "Company Name: Devoteam Middle East\n",
      "Location: Riyadh\n",
      "Scraping terminated before reaching target number of jobs. Needed 20, got 13.\n",
      "postgres://scraper_user:scraper_pass@postgres:5432/job_scraper\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"postgres\" to address: No such host is known. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# args = parse_arguments()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# df = get_jobs(args.job_title, args.num_jobs, args.verbose)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m get_jobs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata engineer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m     \u001b[43minsert_jobs_to_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36minsert_jobs_to_db\u001b[1;34m(jobs_df)\u001b[0m\n\u001b[0;32m      5\u001b[0m     jobs_df[col] \u001b[38;5;241m=\u001b[39m convert_numpy_types(jobs_df[col])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Establish the connection\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnect_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define the insert query\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mconnect_db\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect_db\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATABASE_URL\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDATABASE_URL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AnasM\\OneDrive\\Desktop\\data-scraper\\glassdoor-scraper\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: could not translate host name \"postgres\" to address: No such host is known. \n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='Scrape Glassdoor for job listings.')\n",
    "    parser.add_argument('job_title', type=str, help='Job title to search for')\n",
    "    parser.add_argument('num_jobs', type=int, default=50, help='Number of jobs to scrape')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Increase output verbosity')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = parse_arguments()\n",
    "    # df = get_jobs(args.job_title, args.num_jobs, args.verbose)\n",
    "    df = get_jobs('data engineer', 20, True)\n",
    "\n",
    "    insert_jobs_to_db(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
