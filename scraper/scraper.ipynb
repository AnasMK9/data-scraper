{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_db():\n",
    "    print(os.getenv('DATABASE_URL'))\n",
    "    return psycopg2.connect(os.getenv('DATABASE_URL'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, num_jobs, verbose):\n",
    "    '''Gathers jobs as a dataframe, scraped from Glassdoor'''\n",
    "\n",
    "    # Initializing the webdriver\n",
    "    service = Service(executable_path='./chromedriver.exe')\n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    # Uncomment the line below if you'd like to scrape without a new Chrome window every time.\n",
    "    # options.add_argument('headless')\n",
    "    \n",
    "    # Change the path to where chromedriver is in your home folder.\n",
    "    driver = webdriver.Chrome(options=options, service=service)\n",
    "    driver.set_window_size(1120, 1000)\n",
    "\n",
    "    url = f'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"{keyword}\"&sc.locationSeoString=Riyadh+%28Saudi+Arabia%29&locId=3110290&locT=C'\n",
    "    driver.get(url)\n",
    "    jobs = []\n",
    "\n",
    "    while len(jobs) < num_jobs:  # If true, should be still looking for new jobs.\n",
    "\n",
    "        time.sleep(4)  # Let the page load\n",
    "\n",
    "        # Test for the \"Sign Up\" prompt and get rid of it.\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"selected\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            driver.find_element(By.CLASS_NAME, \"ModalStyle__xBtn___29PT9\").click()  # Close the sign-up modal\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Going through each job in this page\n",
    "        job_cards = driver.find_elements(By.CLASS_NAME, \"jobCard\")  # Updated to match the current HTML structure\n",
    "        \n",
    "        for job_card in job_cards:  \n",
    "            print(f\"Progress: {len(jobs)}/{num_jobs}\")\n",
    "            if len(jobs) >= num_jobs:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                job_title = job_card.find_element(By.CLASS_NAME, \"JobCard_jobTitle___7I6y\").text\n",
    "                company_name = job_card.find_element(By.CLASS_NAME, \"EmployerProfile_compactEmployerName__LE242\").text\n",
    "                location = job_card.find_element(By.CLASS_NAME, \"JobCard_location__rCz3x\").text\n",
    "                job_description = job_card.find_element(By.CLASS_NAME, \"JobCard_jobDescriptionSnippet__yWW8q\").text\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to collect job data: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                salary_estimate = job_card.find_element(By.CLASS_NAME, \"JobCard_salaryEstimate__arV5J\").text\n",
    "            except NoSuchElementException:\n",
    "                salary_estimate = -1\n",
    "\n",
    "            try:\n",
    "                rating = job_card.find_element(By.CLASS_NAME, \"EmployerProfile_ratingContainer__ul0Ef\").text\n",
    "            except NoSuchElementException:\n",
    "                rating = -1\n",
    "\n",
    "            # Printing for debugging\n",
    "            if verbose:\n",
    "                print(f\"Job Title: {job_title}\")\n",
    "                print(f\"Salary Estimate: {salary_estimate}\")\n",
    "                print(f\"Job Description: {job_description[:500]}\")\n",
    "                print(f\"Rating: {rating}\")\n",
    "                print(f\"Company Name: {company_name}\")\n",
    "                print(f\"Location: {location}\")\n",
    "\n",
    "            jobs.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Salary Estimate\": salary_estimate,\n",
    "                \"Job Description\": job_description,\n",
    "                \"Rating\": rating,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location\n",
    "            })\n",
    "\n",
    "        # Clicking on the \"next page\" button\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, './/li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Scraping terminated before reaching target number of jobs. Needed {num_jobs}, got {len(jobs)}.\")\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return pd.DataFrame(jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/20\n",
      "Progress: 1/20\n",
      "Progress: 2/20\n",
      "Progress: 3/20\n",
      "Progress: 4/20\n",
      "Progress: 5/20\n",
      "Progress: 6/20\n",
      "Progress: 7/20\n",
      "Progress: 8/20\n",
      "Progress: 9/20\n",
      "Progress: 10/20\n",
      "Scraping terminated before reaching target number of jobs. Needed 20, got 11.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - AI &amp; Computer Vision Specialist</td>\n",
       "      <td>-1</td>\n",
       "      <td>We are looking for an innovative and experienc...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Ines Partners</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>-1</td>\n",
       "      <td>The ideal candidate will have a strong focus o...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Ines Partners</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist Lead</td>\n",
       "      <td>-1</td>\n",
       "      <td>Develop and maintain best practices for data s...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Giza Systems EG</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>-1</td>\n",
       "      <td>Present information using data visualization t...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Arabic Computer Systems</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>-1</td>\n",
       "      <td>Relevant experience as a data scientist, or in...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Mozn</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>-1</td>\n",
       "      <td>Skilled in utilizing databases, data warehousi...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Mozn</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Exchange and Data Science Specialist</td>\n",
       "      <td>-1</td>\n",
       "      <td>Implement data quality checks and data validat...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Valleysoft</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sales Director- Media Vertical</td>\n",
       "      <td>-1</td>\n",
       "      <td>Advanced Excel and analytical skills are essen...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>ArabyAds</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PowerBI engineer</td>\n",
       "      <td>-1</td>\n",
       "      <td> Background in data warehouse design (e.g. di...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Arabic Computer Systems</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sales Manager- Media Vertical</td>\n",
       "      <td>-1</td>\n",
       "      <td>Advanced Excel and analytical skills are essen...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>ArabyAds</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AI/ML Sales Specialist, MENAT AGS Specialist Team</td>\n",
       "      <td>-1</td>\n",
       "      <td>You conduct compelling executive conversations...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>AWS EMEA SARL (Saudi Arabia Branch)</td>\n",
       "      <td>Riyadh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job Title  Salary Estimate  \\\n",
       "0    Data Scientist - AI & Computer Vision Specialist               -1   \n",
       "1                               Senior Data Scientist               -1   \n",
       "2                                 Data Scientist Lead               -1   \n",
       "3                                      Data Scientist               -1   \n",
       "4                               Senior Data Scientist               -1   \n",
       "5                               Senior Data Scientist               -1   \n",
       "6           Data Exchange and Data Science Specialist               -1   \n",
       "7                      Sales Director- Media Vertical               -1   \n",
       "8                                    PowerBI engineer               -1   \n",
       "9                       Sales Manager- Media Vertical               -1   \n",
       "10  AI/ML Sales Specialist, MENAT AGS Specialist Team               -1   \n",
       "\n",
       "                                      Job Description Rating  \\\n",
       "0   We are looking for an innovative and experienc...     -1   \n",
       "1   The ideal candidate will have a strong focus o...     -1   \n",
       "2   Develop and maintain best practices for data s...    4.1   \n",
       "3   Present information using data visualization t...    2.9   \n",
       "4   Relevant experience as a data scientist, or in...    3.9   \n",
       "5   Skilled in utilizing databases, data warehousi...    3.9   \n",
       "6   Implement data quality checks and data validat...    3.4   \n",
       "7   Advanced Excel and analytical skills are essen...    3.3   \n",
       "8    Background in data warehouse design (e.g. di...    2.9   \n",
       "9   Advanced Excel and analytical skills are essen...    3.3   \n",
       "10  You conduct compelling executive conversations...    3.7   \n",
       "\n",
       "                           Company Name Location  \n",
       "0                         Ines Partners   Riyadh  \n",
       "1                         Ines Partners   Riyadh  \n",
       "2                       Giza Systems EG   Riyadh  \n",
       "3               Arabic Computer Systems   Riyadh  \n",
       "4                                  Mozn   Riyadh  \n",
       "5                                  Mozn   Riyadh  \n",
       "6                            Valleysoft   Riyadh  \n",
       "7                              ArabyAds   Riyadh  \n",
       "8               Arabic Computer Systems   Riyadh  \n",
       "9                              ArabyAds   Riyadh  \n",
       "10  AWS EMEA SARL (Saudi Arabia Branch)   Riyadh  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_jobs(\"data scientist\", 20, False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://scraper_user:scraper_pass@postgres:5432/job_scraper\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"postgres\" to address: No such host is known. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m         conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Insert the scraped job data into the database\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43minsert_jobs_to_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36minsert_jobs_to_db\u001b[1;34m(jobs_df)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_jobs_to_db\u001b[39m(jobs_df):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Establishing the connection\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnect_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Define the insert query\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m, in \u001b[0;36mconnect_db\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect_db\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATABASE_URL\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDATABASE_URL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AnasM\\OneDrive\\Desktop\\data-scraper\\glassdoor-scraper\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: could not translate host name \"postgres\" to address: No such host is known. \n"
     ]
    }
   ],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Function to insert job data into PostgreSQL\n",
    "def insert_jobs_to_db(jobs_df):\n",
    "    # Establishing the connection\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the insert query\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO job_listings (job_title, salary_estimate, job_description, rating, company_name, location)\n",
    "    VALUES %s\n",
    "    ON CONFLICT (job_title, company_name, location) DO NOTHING;\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame rows into tuples\n",
    "    records = jobs_df.to_records(index=False)\n",
    "    records_list = list(records)\n",
    "\n",
    "    try:\n",
    "        # Use execute_values to bulk insert the records\n",
    "        execute_values(cursor, insert_query, records_list)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to insert records: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Insert the scraped job data into the database\n",
    "insert_jobs_to_db(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
